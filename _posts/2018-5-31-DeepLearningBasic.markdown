---
layout:     post
title:      "深度学习基础知识"
subtitle:   "需要了解的关于深度学习的知识和概念"
date:       2018-05-31 12:00:00
author:     "Luo Yifu"
header-img: "img/post-bg-2015.jpg"
tags:
    - deeplearning
---

# 深度学习基础知识

## 符号计算
符号主义的计算首先定义各种变量，然后建立一个“计算图”，计算图规定了各个变量之间的计算关系。建立好的计算图需要编译以确定其内部细节，然而，此时的计算图还是一个“空壳子”，里面没有任何实际的数据，只有当你把需要运算的输入放进去后，才能在整个模型中形成数据流，从而形成输出值。
TensorFlow和Theano都是基于符号计算。

Keras的模型搭建形式也是这种方法，在你搭建Keras模型完毕后，你的模型就是一个空壳子，只有实际生成可调用的函数后（K.function），输入数据，才会形成真正的数据流。

## 张量
张量，或tensor。张量可以看作是向量、矩阵的自然推广，我们用张量来表示广泛的数据类型。

规模最小的张量是0阶张量，即标量，也就是一个数。当我们把一些数有序的排列起来，就形成了1阶张量，也就是一个向量。如果我们继续把一组向量有序的排列起来，就形成了2阶张量，也就是一个矩阵。把矩阵摞起来，就是3阶张量，我们可以称为一个立方体，具有3个颜色通道的彩色图片就是一个这样的立方体。把立方体摞起来，就叫4阶张量了，不要去试图想像4阶张量是什么样子，它就是个数学上的概念。

张量的阶数有时候也称为维度，或者轴，轴这个词翻译自英文axis。譬如一个矩阵[[1,2],[3,4]]，是一个2阶张量，有两个维度或轴，沿着第0个轴（为了与python的计数方式一致，本文档维度和轴从0算起）你看到的是[1,2]，[3,4]两个向量（每一行是一个向量，可以看成是沿x轴看来），沿着第1个轴你看到的是[1,3]，[2,4]两个向量（每一列是一个向量）。

要理解“沿着某个轴”是什么意思，不妨试着运行一下下面的代码：
```
import numpy as np

a = np.array([[1,2],[3,4]])
sum0 = np.sum(a, axis=0)
sum1 = np.sum(a, axis=1)

print sum0
print sum1
```

## Softmax回归
softmax用于多分类过程中，它将多个神经元的输出，映射到（0,1）区间内，可以看成概率来理解，从而来进行多分类。
![img](/img/in-post/softmax.jpg)
以上图为例，softmax直白来说就是将原来输出是3,1,-3通过softmax函数一作用，就映射成为(0,1)的值，而这些值的累和为1（满足概率的性质），那么我们就可以将它理解成概率，在最后选取输出结点的时候，我们就可以选取概率最大（也就是值对应最大的）结点，作为我们的预测目标

TensorFlow官方文档MNIST数据的例子：
MNIST的每一张图片都表示一个数字，从0到9。我们希望得到给定图片代表每个数字的概率。比如说，我们的模型可能推测一张包含9的图片代表数字9的概率是80%但是判断它是8的概率是5%（因为8和9都有上半部分的小圆），然后给予它代表其他数字的概率更小的值。
因此对于给定的输入图片 x 它代表的是数字 i 的证据可以表示为：
evidence_i=sum[x=j](w_ij*x_j+b_i)
y=softmax(evidence)

参考资料
[详解Softmax函数](https://zhuanlan.zhihu.com/p/25723112)

## 梯度下降法
在机器学习的很多**监督学习模型**中，需要对原始模型构建损失函数，然后优化损失函数找到最有参数。在参数优化中，使用较多的是梯度下降算法（Gradient Descent,GD）。
> 直观的理解是：从山上某一点出发，找一个最陡的坡（梯度方向）走一步，到达下一个点后，再找一个最陡的坡，直到走到最低点（最小花费函数收敛点）

## batch
batch可以翻译为“一批，批量”。可以理解为数据的量。
深度学习的优化算法，说白了就是梯度下降(Gradient Descent, GD)。每次的参数更新有3种方式。

1. 批梯度下降法（batch gradient descent, BGD）:遍历全部数据集算一次损失函数，然后算函数对各个参数的梯度，更新梯度。
这种方法每更新一次参数都要把数据集里的所有样本都看一遍，计算量开销大，计算速度慢，不支持在线学习。
> 每迭代一步，都要用到训练集所有的数据，如果样本数目很大，那么可想而知这种方法的迭代速度！ 
> 优点：全局最优解；易于并行实现； 
> 缺点：当样本数目很多时，训练过程会很慢。 

2. 随机梯度下降法（stochastic gradient descent, SGD）:每次仅根据一个样本对模型参数进行调整（即每看一个数据就算一下损失函数，然后求梯度更新参数）。这个方法速度比较快，但是收敛性能不太好，可能在最优点附近晃来晃去，hit不到最优点。两次参数的更新也有可能互相抵消掉，造成目标函数震荡的比较剧烈。
> 优点：训练速度快； 
> 缺点：准确度下降，并不是全局最优；不易于并行实现。 
> 从迭代的次数上来看，SGD迭代的次数较多，在解空间的搜索过程看起来很盲目。

3. 小批量梯度下降法（mini-batch gradient descent, MBGD）:为了克服两种方法的缺点，现在一般采用的是一种折中手段，mini-batch gradient decent，小批的梯度下降，这种方法把数据分为若干个批，按批来更新参数，这样，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。另一方面因为批的样本数与整个数据集相比小了很多，计算量也不是很大。
> 假设训练集中的样本的个数为1000，则每个mini-batch只是其一个子集，假设，每个mini-batch中含有10个样本，这样，整个训练数据集可以分为100个mini-batch。

基本上现在的梯度下降都是基于mini-batch的，所以Keras的模块中经常会出现batch_size，就是指这个。

顺便说一句，Keras中用的优化器SGD是stochastic gradient descent的缩写，但不代表是一个样本就更新一回，还是基于mini-batch的。