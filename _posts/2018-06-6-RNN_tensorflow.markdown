---
layout:     post
title:      "TensorFlow 循环神经网络（RNN）"
subtitle:   ""
date:       2018-06-06 10:00:00
author:     "Luo Yifu"
header-img: "img/post-bg-2015.jpg"
tags:
    - tensorflow
    - deeplearning
---
# TensorFlow循环神经网络（RNN）

## RNN

### RNN思想：

无论是 NN 还是 CNN，它们处理的数据都是相对固定的。NN 自不用提，一旦网络结构确定下来之后，输入数据的维度便随之确定了下来；而虽说 CNN 能够通过 RoI Pooling 等手段来接受不同长宽的图片输入，但它们大多只是在最后一步做出了调整、并没有特别根本地解决了问题。而循环神经网络（Recurrent Neural Network，常简称为 RNN）则通过非常巧妙的形式、让模型能用同一套结构非常自然地处理不同的输入数据，这使得 RNN 在处理序列问题（比如各种 NLP
Tasks）时显得得心应手（注：这并非是绝对的结论，只能说从设计的理念上来看确实更为自然；事实上在特定的序列问题上 CNN 能够表现得比 RNN 更好，比如 Facebook FAIR 团队最近弄出的那个 CNN 翻译模型……）

* 与 CNN 类似，RNN 也可以说是 NN 的一种拓展，但从思想上来说，RNN 和 NN、CNN 相比已经有了相当大的不同：
* NN 只是单纯地接受数据，认为不同样本之间是独立的 CNN 注重于挖掘单个结构性样本（比如说图像）相邻区域之间的关联RNN 注重于挖掘样本与样本之间的序关联（这是我瞎掰的一个词 ( σ'ω')σ）

CNN 是通过局部连接和权值共享来做到相邻区域的特征提取的，那么 RNN 是如何做到提取序特征的呢？关键就在于“状态（State）”的引入。换句话说，在 RNN 中，输入不是简单地通过权值矩阵（NN 的做法）或卷积（CNN 的做法）来得到输出，而是要先得出一个 State、然后再由这个 State 得到输出。这样一来，只需让 State 之间能够“通信”，那么当前样本就能够影响到下一个、乃至于下 n 个样本的 State；由于输出是由 State 决定的，所以影响到了 State 意味着能够影响输出，亦即当前样本能够影响到下一个、乃至于下n 个样本的输出，从而能使结果反映出序特征。这就是 RNN 挖掘序关联的一般性思路。事实上，我们可以把 State 理解为网络结构的“记忆”——它能帮助网络“记住”之前看到过的样本的某些信息，并结合最新的样本所带来的信息来进行决策


* 传统的神经网络是层与层之间是全连接的，但是每层之间的神经元是没有连接的（其实是假设各个数据之间是独立的）
> 这种结构不善于处理序列化的问题。比如要预测句子中的下一个单词是什么，这往往与前面的单词有很大的关联，因为句子里面的单词并不是独立的。
* RNN 的结构说明当前的的输出与前面的输出也有关，即隐层之间的节点不再是无连接的，而是有连接的

基本的结构如图，可以看到有个循环的结构，将其展开就是右边的结构：
![RNN结构图](/img/in-post/rnn.jpg)

如上图：
* 输入单元(inputs units): $\{ {x_0},{x_1}, \cdots \cdots ,{x_t},{x_{t + 1}}, \cdots \cdots \}$,
* 输出单元(output units)为：$\{ {o_0},{o_1}, \cdots \cdots ,{o_t},{o_{t + 1}}, \cdots \cdots \}$,
* 隐藏单元(hidden units)输出集: $\{ {s_0},{s_1}, \cdots \cdots ,{ost},{s_{t + 1}}, \cdots \cdots \}$
* 时间 t 隐层单元的输出为：${s_t} = f(U{x_t} + W{s_{t - 1}})$
> f就是激励函数，一般是sigmoid,tanh, relu等
> 计算${s_{0}}$时，即第一个的隐藏层状态，需要用到${s_{-1}}$，但是其并不存在，在实现中一般置为0向量
> （如果将上面的竖着立起来，其实很像传统的神经网络，哈哈）
* 时间 t 的输出为：${o_t}=Softmax(V{s_t})$
> 可以认为隐藏层状态${s_t}$是网络的记忆单元. ${s_t}$包含了前面所有步的隐藏层状态。而输出层的输出${o_t}$只与当前步的${s_t}$有关。
> （在实践中，为了降低网络的复杂度，往往${s_t}$只包含前面若干步而不是所有步的隐藏层状态）
* 在RNNs中，每输入一步，每一层都共享参数U,V,W，（因为是将循环的部分展开，天然应该相等）
* RNNs的关键之处在于隐藏层，隐藏层能够捕捉序列的信息。


![朴素 RNN 的结构](/img/in-post/rnn2.jpg)
其中，x_{t-1},x_t,x_{t+1}、o_{t-1},o_t,o_{t+1}、s_{t-1},s_t,s_{t+1}可以分别视为第t-1,t,t+1“时刻”的输入、输出与 State。不难看出对于每一个时刻而言，朴素的 RNN 都可视为一个普通的神经网络：

## LSTM


参考资料
[RNN基础知识](http://lawlite.me/2017/06/14/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8CLSTM-01%E5%9F%BA%E7%A1%80/)
[RNN](https://www.jianshu.com/p/9dc9f41f0b29)